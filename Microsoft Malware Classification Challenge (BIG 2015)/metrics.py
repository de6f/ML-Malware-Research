


"""
    Plot ROC curves and calculate AUC scores
    https://scikit-learn.org/0.15/auto_examples/plot_roc.html
"""

def multiclass_roc_auc(y_true, y_score, savefig=False, fig_name=None):
    from sklearn.preprocessing import LabelBinarizer
    from sklearn.metrics import roc_curve, auc
    import matplotlib.pyplot as plt

    lb = LabelBinarizer()
    lb.fit(y_true)
    
    y_true = lb.transform(y_true)
    y_score = lb.transform(y_score)
    
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(y_true.shape[1]):
        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
        
    fpr["micro"], tpr["micro"], _ = roc_curve(y_true.ravel(), y_score.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    
    plt.figure()
    plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
         ''.format(roc_auc["micro"]))
    for i in range(y_true.shape[1]):
        plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'
                 ''.format(i, roc_auc[i]))

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc="lower right")
    if savefig:
        plt.savefig(fig_name)
    #plt.show()
        
    return roc_auc
    
    
    
"""
    Wrapper for sklearn confusion_matrix and seaborn heatmap
"""
    
def confusion_matrix(y_test, y_pred, savefig=False, fig_name=None):
    import seaborn as sn
    import pandas as pd
    from sklearn import metrics
    import matplotlib.pyplot as plt
    
    array = metrics.confusion_matrix(y_test, y_pred)
    
    ls = [i for i in range(set(y_test).__len__())]
    df_cm = pd.DataFrame(array, index=ls, columns=ls)
    plt.figure(figsize =(9,9))
    svm = sn.heatmap(df_cm, annot=True)
    
    if savefig:
        figure = svm.get_figure()
        figure.savefig(fig_name)
        
        
"""
    Decision tree visualizing using Graphviz
    https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176
"""

def dtree_viz(model):
    import pydotplus
    from io import StringIO
    from sklearn.tree import export_graphviz
    
    dot_data = StringIO()
    export_graphviz(model, out_file=dot_data, 
                    filled=True, rounded=True,
                    special_characters=True)
    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
    graph.write_png('dt.png')


"""
    Classification report with logging
"""

from numpy import argmax, math
from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

class TestReport:
    def __init__(self, savefig, average='macro'):
        self.savefig = savefig
        self.average = average                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         
        self.reports = list()
        
        metrics = ['avg_acc', 'avg_precision', 'avg_recall', 'avg_fscore',
                   'support']
        self.overall_report = dict.fromkeys(metrics)
        

    def __call__(self, model, test, print_report=False, softmax=False):
        X_test = test[0]
        y_test = test[1]

        y_pred = model.predict(X_test)
        y_test = argmax(y_test, axis=1)
        
        if softmax:
            y_pred = argmax(y_pred, axis=1)

        roc_plot_name = 'ROC_fold_{}.png'.format(self.__len__())
        auc = multiclass_roc_auc(y_test, y_pred, self.savefig, roc_plot_name)
        
        cm_name = 'CM_fold_{}.png'.format(self.__len__())
        confusion_matrix(y_test, y_pred, True, cm_name)
        
        report = self.classification_eval(y_test, y_pred)
        report['roc_auc'] = auc['micro']
        
        if print_report:
            self.print_classification_report(y_test, y_pred, report['acc'])

        self.reports.append(report)


    def __len__(self):
        return len(self.reports)
    
    
    def __add__(self, other):
        self.reports = self.reports + other.reports
        return self
    
    def predict_generator(self, model, test_gen, print_report=False):
        test_steps_per_epoch = math.ceil(test_gen.samples / test_gen.batch_size)
        pred = model.predict_generator(test_gen, steps=test_steps_per_epoch)
        
        y_pred = argmax(pred, axis=1)
        y_test = test_gen.classes
        
        #class_labels = list(test_gen.class_indices.keys())
        
        roc_plot_name = 'ROC_fold_{}.png'.format(self.__len__())
        auc = multiclass_roc_auc(y_test, y_pred, self.savefig, roc_plot_name)
        
        cm_name = 'CM_fold_{}.png'.format(self.__len__())
        confusion_matrix(y_test, y_pred, True, cm_name)
        
        report = self.classification_eval(y_test, y_pred)
        report['roc_auc'] = auc['micro']
        
        if print_report:
            self.print_classification_report(y_test, y_pred, report['acc'])

        self.reports.append(report)

    def classification_eval(self, y_test, y_pred):
        precision, recall, fscore, support = score(y_test, y_pred,
                                                   average=self.average)
        return {'acc': accuracy_score(y_test, y_pred),
                'precision': precision,
                'recall': recall,
                'fscore': fscore,
                'support': support,
                'roc_auc': .0}
        
        
    def print_classification_report(self, y_test, y_pred, acc):
        print('\nModel accuracy: ' + str(acc))
        print('\n\n')
        print(classification_report(y_test, y_pred, digits=5))

    
    def make_report(self):
        fold = 0
        acc = 0
        precision = 0
        recall = 0
        fscore = 0
        auc = 0
        for report in self.reports:
            fold = fold + 1
            acc = acc + report['acc']
            precision = precision + report['precision']
            recall = recall + report['recall']
            fscore = fscore + report['fscore']
            auc = auc + report['roc_auc']

        self.overall_report['avg_acc'] = acc / fold            
        self.overall_report['avg_precision'] = precision / fold
        self.overall_report['avg_recall'] = recall / fold
        self.overall_report['avg_fscore'] = fscore / fold
        self.overall_report['support'] = report['support']
        self.overall_report['roc_auc'] = auc / fold 
        
        return self.overall_report
